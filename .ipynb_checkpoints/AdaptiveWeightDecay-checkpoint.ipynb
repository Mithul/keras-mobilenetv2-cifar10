{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.scroll_box { height:1000em  !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import shutil\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from qctools.mask_layer import MaskExponentialAlphaScheduler, MaskCustomCSVLogger  #  CustomCallback class\n",
    "from models import cnn_base, cnn_qc, cnn_qcsc\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.scroll_box { height:1000em  !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - WeightDecay (L2)\n",
    "\n",
    "モデルの損失関数$L({\\bf w}) $は，タスクに対する損失関数$\\rho({\\bf w})$とL2正則化項$\\frac{\\lambda}{2} {||{\\bf w}||}^{2}_{2} $からなる．\n",
    "\n",
    "$$\n",
    "{{\\bf w}}^{*} = \\underset{{\\bf w}}{\\rm argmin} ~ L({\\bf w}) \\\\\n",
    "L({\\bf w}) = \\rho({\\bf w}) + \\frac{\\lambda}{2} {||{\\bf w}||}^{2}_{2} \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf w}^{new}\n",
    "&\\gets {\\bf w} - \\eta \\cdot \\nabla_{{\\bf w}} L({\\bf w}) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\nabla_{{\\bf w}} \\left( \\rho({\\bf w}) + \\frac{\\lambda}{2} {||{\\bf w}||}^{2}_{2}  \\right) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\left( \\nabla_{{\\bf w}}\\rho({\\bf w}) + \\nabla_{{\\bf {\\bf w}}}\\frac{\\lambda}{2} {||{\\bf w}||}^{2}_{2} \\right) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\left( \\nabla_{w}\\rho({\\bf w}) + \\lambda {\\bf w} \\right) \n",
    "\\\\ \\\\\n",
    "&= \\left( 1 - \\eta \\lambda \\right) {\\bf w} - \\eta \\cdot \\nabla_{{\\bf w}}\\rho({\\bf w}) \n",
    "\\\\ \\\\\n",
    "&= \\left( 1 - \\eta \\lambda \\right) \\left( \\begin{array}{c} w_1 \\\\ \\vdots \\\\  w_m \\end{array} \\right)\n",
    "- \\eta \\cdot \\nabla_{{\\bf w}}\\rho({\\bf w})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### SGD - AdaptiveWeightDecay (L2)\n",
    "\n",
    "SGDのL2-正則化項に対して，「パラメータ$\\bf w$のL2ノルム」を，「パラメータ$\\bf w$の重みベクトル$\\theta$との内積」で置き換えることで，各パラメータ$w_j$に対する正則化係数を動的に変化させる．\n",
    "\n",
    "\n",
    "$$\n",
    "{{\\bf w}}^{*} = \\underset{{\\bf w}}{\\rm argmin} ~ L({\\bf w}) \\\\\n",
    "L({\\bf w}) = \\rho({\\bf w}) + \\frac{\\lambda}{2} {||{\\bf \\theta}^{\\mathrm{T}}{\\bf w}||}^{2}_{2} \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\bf w}^{new}\n",
    "&\\gets {\\bf w} - \\eta \\cdot \\nabla_{{\\bf w}} L({\\bf w}) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\nabla_{{\\bf w}} \\left( \\rho({\\bf w}) + \\frac{\\lambda}{2} {||{\\bf \\theta}^{\\mathrm{T}}{\\bf w}||}^{2}_{2}   \\right) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\left( \\nabla_{{\\bf w}}\\rho({\\bf w}) + \\nabla_{{\\bf {\\bf w}}}\\frac{\\lambda}{2} {||{\\bf \\theta}^{\\mathrm{T}}{\\bf w}||}^{2}_{2}  \\right) \n",
    "\\\\\n",
    "&= {\\bf w} - \\eta \\cdot \\left( \\nabla_{w}\\rho({\\bf w}) + \\lambda ({\\bf \\theta \\odot w}) \\right) \n",
    "\\\\ \\\\\n",
    "&= \\left( 1 - \\eta \\lambda \\right) \\left( \\begin{array}{c} \\theta_1w_1 \\\\ \\vdots \\\\ \\theta_m w_m \\end{array} \\right)\n",
    "- \\eta \\cdot \\nabla_{{\\bf w}}\\rho({\\bf w}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "重みベクトル$\\theta$は，モデルの損失関数$\\rho(w)$との勾配を，レイヤーごとに標準化した後，Sigmoidで[0,1]にスケーリングした値として決定される．\n",
    "\n",
    "$\\mu_{\\ell}, ~ \\sigma^{2}_{\\ell}$ は，$\\ell$-th 層を構成する$g_j$（＝パラメータ$w_j$の微分値）の平均と分散\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_j &= {\\rm Sigmoid}({\\tilde{g}}_{j} ; \\alpha) = \\frac{2}{1 + \\exp (-\\alpha {\\tilde{g}}_{j}) }\n",
    "\\\\ \\\\\n",
    "{\\tilde{g}}_{j} &=  \\frac{|g_j| - \\mu_{\\ell}}{\\sigma_{\\ell}}, ~~~\n",
    "g_j = \\frac{\\partial \\rho({\\bf w})}{\\partial w_j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "- \"Adaptive Weight Decay for Deep Neural Networks\"<br>https://arxiv.org/pdf/1907.08931.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    \"\"\"\n",
    "    keras.optimizer.Adagrad\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n",
    "        super(Adagrad, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        accumulators = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = accumulators\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        for p, g, a in zip(params, grads, accumulators):\n",
    "            new_a = a + K.square(g)  # update accumulator\n",
    "            self.updates.append(K.update(a, new_a))\n",
    "            new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(Adagrad, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-6be8dc207156>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-6be8dc207156>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    for p, g, m a in zip(params, grads, moments, accumulators):\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Optimizer\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class SGDAdaptiveWeightDecay(Optimizer):\n",
    "    \"\"\"Stochastic gradient descent optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0., decay=0., alpha=1.0,\n",
    "                 nesterov=False, **kwargs):\n",
    "        super(SGDAdaptiveWeightDecay, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.momentum = K.variable(momentum, name='momentum')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.alpha = K.variable(alpha, name='alpha')\n",
    "        self.initial_decay = decay\n",
    "        self.nesterov = nesterov    \n",
    "        \n",
    "    def calc_theta(self, grads, alpha=1.0):\n",
    "        print(\"calc_theta is called\")\n",
    "        def sigmoid(x):\n",
    "            return 2.0 / (1.0 + np.exp(-x * alpha))\n",
    "        theta=[]\n",
    "        for i, g in enumerate(grads):\n",
    "            # g は各レイヤーのパラメータに対応する勾配テンソル\n",
    "            # print(\"type(g)\", type(g))\n",
    "            \n",
    "            g_mean = K.mean(g)\n",
    "            g_std = K.std(g)\n",
    "            g_ = tf.div(tf.subtract(g, g_mean), g_std)\n",
    "            theta_i = K.map_fn(lambda x: tf.math.sigmoid(x), g_, name='theta_'+str(i))\n",
    "            theta.append(theta_i)\n",
    "        theta = np.array(theta)\n",
    "        return theta\n",
    "            \n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):    \n",
    "        self.params = params\n",
    "        self.grads = self.get_gradients(loss, params)\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        accumulators = [K.zeros(shape) for shape in shapes]\n",
    "        # self.weights = accumulators\n",
    "        \n",
    "        # debug\n",
    "        # print(\"optimizer.get_updates() is called!\")\n",
    "        \n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        \n",
    "        # debug code -----------------------------------\n",
    "        # self.theta = self.calc_theta(grads)\n",
    "        # self.grads = grads\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        # lr decay\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
    "            \n",
    "        # momentum\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "\n",
    "        for p, g, m a in zip(params, grads, moments, accumulators):\n",
    "            new_a = self.calc_theta(g)  # update accumulator\n",
    "            self.updates.append(K.update(a, new_a))\n",
    "            \n",
    "            v = self.momentum * m - lr * g  # velocity\n",
    "            self.updates.append(K.update(m, v)) # update momentum\n",
    "\n",
    "            if self.nesterov:\n",
    "                new_p = p + self.momentum * v - lr * g\n",
    "            else:\n",
    "                new_p = p + v\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'momentum': float(K.get_value(self.momentum)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'nesterov': self.nesterov}\n",
    "        base_config = super(SGDAdaptiveWeightDecay, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class OptimizerHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        print(\"epoch : \"+str(epoch), \"`on_batch_end()` is called.\")\n",
    "        \n",
    "        print()\n",
    "        print(\"model.optimizer.grads\")\n",
    "        pprint(self.model.optimizer.grads)\n",
    "        print()\n",
    "        print(\"model.optimizer.theta\")\n",
    "        pprint(self.model.optimizer.theta)\n",
    "        print()\n",
    "        print(\"model.losses\")\n",
    "        pprint(self.model.losses)\n",
    "        print()\n",
    "        print(\"model.total_loss\")\n",
    "        pprint(self.model.total_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "class AdaWeightDecay(Callback):\n",
    "    '''\n",
    "    Custom `keras.callbacks.Callback` class for regularizing QcConv2D layer \n",
    "    that control the value of alpha for each epoch.\n",
    "\n",
    "    Attributes\n",
    "    ==========\n",
    "    base_alpha : float\n",
    "        value of alpha in first epoch\n",
    "    epoch : int\n",
    "        current epoch\n",
    "    growth_steps : int\n",
    "        `base_alpha` is multiplied by `growth_rate` in `growth_steps` steps\n",
    "    growth_rate : int\n",
    "        `base_alpha` is multiplied by `growth_rate` in `growth_steps` steps\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                init_alpha,\n",
    "                name=None):\n",
    "        \n",
    "        self.init_alpha = init_alpha\n",
    "        self.name = name\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        new_alpha = self.alpha_exponential_growth(epoch)\n",
    "        logs[\"alpha\"] = new_alpha\n",
    "        for lay in self.model.layers:\n",
    "            if hasattr(lay, 'kernel_regularizer') and hasattr(lay.kernel_regularizer, 'alpha'):\n",
    "                K.set_value(lay.kernel_regularizer.alpha, new_alpha)\n",
    "                # debug code\n",
    "                # print(\"epoch :\", epoch, \" layer name :\", lay.name, \" alpha :\", K.eval(lay.kernel_regularizer.alpha), flush=True) # for debug\n",
    "\n",
    "    def calc_theta(self, grads, alpha=1.0):\n",
    "        print(\"calc_theta is called\")\n",
    "        def sigmoid(x):\n",
    "            return 2.0 / (1.0 + np.exp(-x * alpha))\n",
    "        theta=[]\n",
    "        for i, g in enumerate(grads):\n",
    "            # g は各レイヤーのパラメータに対応する勾配テンソル\n",
    "            # print(\"type(g)\", type(g))\n",
    "            \n",
    "            g_mean = K.mean(g)\n",
    "            g_std = K.std(g)\n",
    "            g_ = tf.div(tf.subtract(g, g_mean), g_std)\n",
    "            theta_i = K.map_fn(lambda x: tf.math.sigmoid(x), g_, name='theta_'+str(i))\n",
    "            theta.append(theta_i)\n",
    "        theta = np.array(theta)\n",
    "        return theta \n",
    "                \n",
    "    def alpha_exponential_growth(self, epoch):\n",
    "        grads = self.model.optimizer.grads\n",
    "        # params = self.model.optimizer.params\n",
    "        # shapes = [K.int_shape(p) for p in params]\n",
    "        theta = self.calc_theta(grads)\n",
    "        \n",
    "        theta * grads\n",
    "        \n",
    "        p = epoch / self.growth_steps\n",
    "        if self.staircase:\n",
    "            p = math_ops.floor(p)\n",
    "        new_alpha = self.init_alpha * math.pow(self.growth_rate, p)\n",
    "\n",
    "        if new_alpha > self.clip_max:\n",
    "            new_alpha = self.clip_max\n",
    "        elif new_alpha < self.clip_min:\n",
    "            new_alpha = self.clip_min\n",
    "            \n",
    "        return new_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with K.name_scope('training'):\n",
    "                with K.name_scope(self.optimizer.__class__.__name__):\n",
    "                    training_updates = self.optimizer.get_updates(\n",
    "                        params=self._collected_trainable_weights,\n",
    "                        loss=self.total_loss)\n",
    "                updates = (self.updates +\n",
    "                           training_updates +\n",
    "                           self.metrics_updates)\n",
    "                # Gets loss and metrics. Updates weights at each call.\n",
    "                self.train_function = K.function(\n",
    "                    inputs,\n",
    "                    [self.total_loss] + self.metrics_tensors,\n",
    "                    updates=updates,\n",
    "                    name='train_function',\n",
    "                    **self._function_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "QC-Conv_1 (MaskConv2D)       (None, 26, 26, 32)        288       \n",
      "_________________________________________________________________\n",
      "QC-Conv_2 (MaskConv2D)       (None, 24, 24, 32)        9216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "QC-Conv_3 (MaskConv2D)       (None, 12, 12, 64)        18432     \n",
      "_________________________________________________________________\n",
      "QC-Conv_4 (MaskConv2D)       (None, 12, 12, 64)        36864     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "QC-Conv_5 (MaskConv2D)       (None, 6, 6, 128)         73728     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "QC-Dense_1 (MaskDense)       (None, 128)               589824    \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 10)                1280      \n",
      "=================================================================\n",
      "Total params: 729,632\n",
      "Trainable params: 729,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "optimizer.get_updates() is called!\n",
      "calc_theta is called\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 3.5667 - acc: 0.2779 - val_loss: 0.7387 - val_acc: 0.7973\n",
      "epoch : 0 `on_batch_end()` is called.\n",
      "\n",
      "model.optimizer.grads\n",
      "[<tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/AddN_4:0' shape=(3, 3, 1, 32) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/AddN_3:0' shape=(3, 3, 32, 32) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/AddN_2:0' shape=(3, 3, 32, 64) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/AddN_1:0' shape=(3, 3, 64, 64) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/AddN:0' shape=(3, 3, 64, 128) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/QC-Dense_1_1/MatMul_grad/MatMul_1:0' shape=(4608, 128) dtype=float32>,\n",
      " <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/gradients/Dense_2_1/MatMul_grad/MatMul_1:0' shape=(128, 10) dtype=float32>]\n",
      "\n",
      "model.optimizer.theta\n",
      "array([<tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_0/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3, 1, 32) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_1/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3, 32, 32) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_2/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3, 32, 64) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_3/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3, 64, 64) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_4/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3, 64, 128) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_5/TensorArrayStack/TensorArrayGatherV3:0' shape=(4608, 128) dtype=float32>,\n",
      "       <tf.Tensor 'training_1/SGDAdaptiveWeightDecay/theta_6/TensorArrayStack/TensorArrayGatherV3:0' shape=(128, 10) dtype=float32>],\n",
      "      dtype=object)\n",
      "\n",
      "model.losses\n",
      "[<tf.Tensor 'QC-Conv_1_1/weight_regularizer/Sum:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'QC-Conv_2_1/weight_regularizer/Sum:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'QC-Conv_4_1/weight_regularizer/Sum:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'QC-Conv_3_1/weight_regularizer/Sum:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'QC-Conv_5_1/weight_regularizer/Sum:0' shape=() dtype=float32>]\n",
      "\n",
      "model.total_loss\n",
      "<tf.Tensor 'loss_1/add_4:0' shape=() dtype=float32>\n",
      "\n",
      "Epoch 2/10\n",
      "28032/60000 [=============>................] - ETA: 4s - loss: 0.3095 - acc: 0.9075"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-024aebdb6847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m               validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "(X_train, y_train, X_test, y_test) = X_train[:, :, :, np.newaxis], y_train[:, np.newaxis], X_test[:, :, :, np.newaxis], y_test[:, np.newaxis]\n",
    "\n",
    "# Normalization\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Define model\n",
    "model = cnn_qc()\n",
    "\n",
    "# Compile model \n",
    "model.summary()\n",
    "model.compile(optimizer=SGDAdaptiveWeightDecay(lr=2e-2, momentum=0.9, decay=0.0, nesterov=False), # SGD(lr=2e-2, momentum=0.9, decay=0.0, nesterov=False)\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              loss_weights=[1.0], # The loss weight for model output without regularization loss. Set 0.0 due to validate only regularization factor.\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set model callbacks\n",
    "callbacks = []\n",
    "callbacks.append(OptimizerHistory())\n",
    "callbacks.append(MaskExponentialAlphaScheduler(init_alpha=1.0, growth_steps=10, growth_rate=10, clip_min=0, clip_max=100))\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, \n",
    "              y_train, \n",
    "              batch_size=128, \n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<tf.Tensor 'training/SGDAdaptiveWeightDecay/gradients/AddN_3\n",
    "\n",
    "<tf.Tensor 'training/SGDAdaptiveWeightDecay/gradients/\n",
    "QC-Dense_1/MatMul_grad/MatMul_1:0' shape=(4608, 128) dtype=float32>,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_theta(grads, alpha=1.0):\n",
    "    def sigmoid(x):\n",
    "        return 2.0 / (1.0 + np.exp(-x * alpha))\n",
    "    vsigmoid=np.vectorize(sigmoid)\n",
    "    grads = K.get_value(grads)\n",
    "    theta=[]\n",
    "    for g in grads:\n",
    "        # g は各レイヤーのパラメータに対応する勾配テンソル\n",
    "        g_std = (g - np.mean(g)) / np.std(g)\n",
    "        theta_i = vsigmoid(g_std)\n",
    "        theta.append(theta_i)\n",
    "    theta = np.array(theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tf.constant([[1,2,3,4,5], [1,2,3,4,5]])\n",
    "calc_theta(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_theta(grads, alpha=1.0):\n",
    "    def sigmoid(x):\n",
    "        return 2.0 / (1.0 + np.exp(-x * alpha))\n",
    "    vsigmoid=np.vectorize(sigmoid)\n",
    "    # grads = K.get_value(grads)\n",
    "    theta=[]\n",
    "    for g in grads:\n",
    "        print(type(g))\n",
    "        print(K.get_value(g))\n",
    "        g = K.get_value(g)\n",
    "\n",
    "        # g は各レイヤーのパラメータに対応する勾配テンソル\n",
    "        g_std = (g - np.mean(g)) / np.std(g)\n",
    "        theta_i = vsigmoid(g_std)\n",
    "        theta.append(theta_i)\n",
    "    theta = np.array(theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_theta(grads, alpha=1.0):\n",
    "    def sigmoid(x):\n",
    "        return 2.0 / (1.0 + np.exp(-x * alpha))\n",
    "    vsigmoid=np.vectorize(sigmoid)\n",
    "    theta=[]\n",
    "    for g in grads:\n",
    "        print(type(g))\n",
    "        K.update_sub(g, K.mean(g))\n",
    "        K.update_div(g, K.std(g))\n",
    "\n",
    "        # g は各レイヤーのパラメータに対応する勾配テンソル\n",
    "        g_std = (g - np.mean(g)) / np.std(g)\n",
    "        theta_i = vsigmoid(g_std)\n",
    "        theta.append(theta_i)\n",
    "    theta = np.array(theta)\n",
    "    return theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
